{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xoaryaa/NER-in-Indian-Headlines/blob/main/aist_conf_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOAf9JqQGhMo"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets flair spacy seqeval stanza scikit-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rkdmMAqHk67"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "def load_labelstudio_json(path):\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dataset = []\n",
        "    for entry in data:\n",
        "        text = entry['data']['text']\n",
        "        labels = entry['annotations'][0]['result']\n",
        "\n",
        "        entities = []\n",
        "        for label in labels:\n",
        "            start = label['value']['start']\n",
        "            end = label['value']['end']\n",
        "            entity = label['value']['labels'][0]\n",
        "            entities.append((start, end, entity))\n",
        "\n",
        "        dataset.append({\n",
        "            'text': text,\n",
        "            'entities': entities\n",
        "        })\n",
        "    return dataset\n",
        "\n",
        "dataset = load_labelstudio_json(\"annotated_ner-238.json\")\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzOCZUTXHvee"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# def convert_to_bio(dataset):\n",
        "#     bio_data = []\n",
        "\n",
        "#     for item in dataset:\n",
        "#         text = item[\"text\"]\n",
        "#         entities = item[\"entities\"]\n",
        "#         tokens = word_tokenize(text)\n",
        "#         tags = [\"O\"] * len(tokens)\n",
        "\n",
        "#         for start, end, label in entities:\n",
        "#             span_text = text[start:end]\n",
        "#             span_tokens = word_tokenize(span_text)\n",
        "#             for i, tok in enumerate(tokens):\n",
        "#                 if tok == span_tokens[0] and tokens[i:i+len(span_tokens)] == span_tokens:\n",
        "#                     tags[i] = \"B-\" + label\n",
        "#                     for j in range(1, len(span_tokens)):\n",
        "#                         if i + j < len(tags):\n",
        "#                             tags[i + j] = \"I-\" + label\n",
        "#                     break\n",
        "#         bio_data.append(list(zip(tokens, tags)))\n",
        "\n",
        "#     return bio_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFRxQ9YDHyVU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "bert_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "bert_pipe = pipeline(\"ner\", model=bert_model, tokenizer=bert_tokenizer, aggregation_strategy=\"simple\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvVwadxwH096"
      },
      "outputs": [],
      "source": [
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "flair_tagger = SequenceTagger.load(\"ner\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjj-LDRUH3of"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyjBQpWnH8nB"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "stanza.download(\"en\")\n",
        "nlp_stanza = stanza.Pipeline(\"en\", processors=\"tokenize,ner\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPpJBKKqbjRg"
      },
      "outputs": [],
      "source": [
        "def convert_to_bio(dataset, tokenizer=bert_tokenizer):\n",
        "    bio_data = []\n",
        "\n",
        "    for item in dataset:\n",
        "        text = item[\"text\"]\n",
        "        entities = item[\"entities\"]\n",
        "        encoding = tokenizer(text, return_offsets_mapping=True)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
        "        offsets = encoding[\"offset_mapping\"]\n",
        "        tags = [\"O\"] * len(tokens)\n",
        "\n",
        "        for start, end, label in entities:\n",
        "            for i, (token_start, token_end) in enumerate(offsets):\n",
        "                if token_start == start:\n",
        "                    tags[i] = \"B-\" + label\n",
        "                elif token_start > start and token_end <= end:\n",
        "                    tags[i] = \"I-\" + label\n",
        "\n",
        "        bio_data.append(list(zip(tokens, tags)))\n",
        "\n",
        "    return bio_data\n",
        "bio_data = convert_to_bio(dataset)\n",
        "print(bio_data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP9lyc_WM5b4"
      },
      "outputs": [],
      "source": [
        "def normalize_label(label):\n",
        "    mapping = {\n",
        "        \"PER\": \"PERSON\",\n",
        "        \"PERSON\": \"PERSON\",\n",
        "        \"ORG\": \"ORG\",\n",
        "        \"ORGANIZATION\": \"ORG\",\n",
        "        \"GPE\": \"LOC\",\n",
        "        \"LOCATION\": \"LOC\",\n",
        "        \"LOC\": \"LOC\",\n",
        "        \"MISC\": \"MISC\",\n",
        "        \"EVENT\": \"EVENT\",\n",
        "        \"NORP\": \"MISC\",\n",
        "        \"CARDINAL\": \"MISC\",\n",
        "        \"PRODUCT\": \"MISC\",\n",
        "        \"DATE\": \"MISC\"\n",
        "    }\n",
        "    return mapping.get(label.upper(), label.upper())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATFMrwbtH-y9"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# bert_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "\n",
        "# def predict_with_bert(text):\n",
        "#     results = bert_pipeline(text)\n",
        "#     entities = []\n",
        "#     for ent in results:\n",
        "#         start, end = ent['start'], ent['end']\n",
        "#         label = normalize_label(ent['entity_group'])\n",
        "#         entities.append((start, end, label))\n",
        "#     return entities\n",
        "\n",
        "\n",
        "# bert_preds = []\n",
        "# for item in dataset:\n",
        "#     text = item['text']\n",
        "#     preds = predict_with_bert(text, bert_pipe)\n",
        "#     bert_preds.append({'text': text, 'predicted_entities': preds})\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "bert_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "bert_ner = pipeline(\"ner\", model=bert_model, tokenizer=bert_tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "def predict_with_bert(text):\n",
        "    preds = bert_ner(text)\n",
        "    entities = []\n",
        "    for ent in preds:\n",
        "        entities.append((ent[\"start\"], ent[\"end\"], ent[\"entity_group\"]))\n",
        "    return entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIscuibiKPdl"
      },
      "outputs": [],
      "source": [
        "# from flair.models import SequenceTagger\n",
        "# from flair.data import Sentence\n",
        "\n",
        "# flair_tagger = SequenceTagger.load(\"flair/ner-english\")\n",
        "\n",
        "# def predict_with_flair(text):\n",
        "#     sentence = Sentence(text)\n",
        "#     flair_tagger.predict(sentence)\n",
        "#     entities = []\n",
        "#     for entity in sentence.get_spans('ner'):\n",
        "#         start = entity.start_position\n",
        "#         end = entity.end_position\n",
        "#         label = normalize_label(entity.get_label(\"ner\").value)\n",
        "#         entities.append((start, end, label))\n",
        "#     return entities\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "flair_model = SequenceTagger.load(\"ner\")\n",
        "\n",
        "def predict_with_flair(text):\n",
        "    sentence = Sentence(text)\n",
        "    flair_model.predict(sentence)\n",
        "    entities = []\n",
        "    for entity in sentence.get_spans('ner'):\n",
        "        entities.append((entity.start_position, entity.end_position, entity.tag))\n",
        "    return entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXd-Pf15KWZb"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# def predict_with_spacy(text):\n",
        "#     doc = nlp_spacy(text)\n",
        "#     entities = []\n",
        "#     for ent in doc.ents:\n",
        "#         label = normalize_label(ent.label_)\n",
        "#         entities.append((ent.start_char, ent.end_char, label))\n",
        "#     return entities\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")  # or custom model\n",
        "\n",
        "def predict_with_spacy(text):\n",
        "    doc = nlp_spacy(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        # Convert entity to format (start, end, label)\n",
        "        entities.append((ent.start_char, ent.end_char, ent.label_))\n",
        "    return entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR0wK3wLKYr5"
      },
      "outputs": [],
      "source": [
        "# import stanza\n",
        "# stanza.download('en')\n",
        "# nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
        "\n",
        "# def predict_with_stanza(text):\n",
        "#     doc = nlp_stanza(text)\n",
        "#     entities = []\n",
        "#     for sent in doc.sentences:\n",
        "#         for ent in sent.ents:\n",
        "#             start = ent.start_char\n",
        "#             end = ent.end_char\n",
        "#             label = normalize_label(ent.type)\n",
        "#             entities.append((start, end, label))\n",
        "#     return entities\n",
        "\n",
        "import stanza\n",
        "\n",
        "stanza.download('en')  # only once\n",
        "stanza_nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
        "\n",
        "def predict_with_stanza(text):\n",
        "    doc = stanza_nlp(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append((ent.start_char, ent.end_char, ent.type))\n",
        "    return entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY1tpz7OMOUh"
      },
      "outputs": [],
      "source": [
        "sample = \"Narendra Modi visited Ayodhya during Diwali celebrations.\"\n",
        "\n",
        "print(\"BERT:\", predict_with_bert(sample))\n",
        "print(\"spaCy:\", predict_with_spacy(sample))\n",
        "print(\"Flair:\", predict_with_flair(sample))\n",
        "print(\"Stanza:\", predict_with_stanza(sample))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oH7hbClKbVY"
      },
      "outputs": [],
      "source": [
        "def normalize_label(label, model=None):\n",
        "    # Standardize label formatting\n",
        "    label = label.upper().replace(\"B-\", \"\").replace(\"I-\", \"\").replace(\"S-\", \"\").replace(\"E-\", \"\")\n",
        "\n",
        "    if model in [\"bert\", \"flair\"]:\n",
        "        if label in [\"PER\", \"PERSON\"]:\n",
        "            return \"PERSON\"\n",
        "        elif label in [\"LOC\", \"GPE\"]:\n",
        "            return \"LOC\"\n",
        "        elif label == \"ORG\":\n",
        "            return \"ORG\"\n",
        "        elif label == \"EVENT\":\n",
        "            return \"EVENT\"\n",
        "        elif label == \"MISC\":\n",
        "            return \"MISC\"\n",
        "        else:\n",
        "            return \"O\"\n",
        "\n",
        "    elif model == \"spacy\":\n",
        "        spacy_map = {\n",
        "            \"PERSON\": \"PERSON\",\n",
        "            \"LOC\": \"LOC\", \"GPE\": \"LOC\",\n",
        "            \"ORG\": \"ORG\",\n",
        "            \"EVENT\": \"EVENT\",\n",
        "            \"NORP\": \"MISC\", \"DATE\": \"MISC\", \"CARDINAL\": \"MISC\",\n",
        "            \"PRODUCT\": \"MISC\", \"FAC\": \"MISC\", \"LANGUAGE\": \"MISC\"\n",
        "        }\n",
        "        return spacy_map.get(label, \"O\")\n",
        "\n",
        "    elif model == \"stanza\":\n",
        "        stanza_map = {\n",
        "            \"PERSON\": \"PERSON\",\n",
        "            \"ORG\": \"ORG\",\n",
        "            \"GPE\": \"LOC\", \"LOC\": \"LOC\",\n",
        "            \"EVENT\": \"EVENT\"\n",
        "        }\n",
        "        return stanza_map.get(label, \"MISC\")\n",
        "\n",
        "    return \"O\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_tEYfsQcACH"
      },
      "outputs": [],
      "source": [
        "sample1 = \"NaMo visited Ayodhya during Diwali celebrations.\"\n",
        "\n",
        "print(\"BERT:\", predict_with_bert(sample1))\n",
        "print(\"spaCy:\", predict_with_spacy(sample1))\n",
        "print(\"Flair:\", predict_with_flair(sample1))\n",
        "print(\"Stanza:\", predict_with_stanza(sample1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3Umu1-VObQJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load your annotated dataset\n",
        "with open(\"annotated_ner-238.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Format check (optional, just to see one sample)\n",
        "print(raw_data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmdnO6R1ZdYm"
      },
      "outputs": [],
      "source": [
        "def convert_labelstudio_to_eval_format(labelstudio_data):\n",
        "    converted = []\n",
        "    for item in labelstudio_data:\n",
        "        text = item.get(\"data\", {}).get(\"text\", \"\")\n",
        "        entities = []\n",
        "\n",
        "        for ann in item.get(\"annotations\", []):\n",
        "            for result in ann.get(\"result\", []):\n",
        "                start = result[\"value\"][\"start\"]\n",
        "                end = result[\"value\"][\"end\"]\n",
        "                label = result[\"value\"][\"labels\"][0]  # assuming one label\n",
        "                entities.append((start, end, label))\n",
        "\n",
        "        converted.append({\n",
        "            \"text\": text,\n",
        "            \"entities\": entities\n",
        "        })\n",
        "\n",
        "    return converted\n",
        "\n",
        "# Apply conversion\n",
        "annotated_data = convert_labelstudio_to_eval_format(raw_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9wMFWqgLHWF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from collections import defaultdict\n",
        "\n",
        "# def evaluate_model(model_name, predict_fn, annotated_data):\n",
        "#     all_true = []\n",
        "#     all_pred = []\n",
        "\n",
        "#     for item in annotated_data:\n",
        "#         text = item['text']\n",
        "#         true_entities = item.get('entities', [])  # format: [(start, end, label)]\n",
        "\n",
        "#         pred_entities = predict_fn(text)  # format: [(start, end, label)]\n",
        "\n",
        "#         # Convert to label sets for span matching\n",
        "#         true_set = set((s, e, normalize_label(l)) for s, e, l in true_entities)\n",
        "#         pred_set = set((s, e, normalize_label(l)) for s, e, l in pred_entities)\n",
        "\n",
        "# def evaluate_model(model_name, predict_fn, annotated_data):\n",
        "#     all_true = []\n",
        "#     all_pred = []\n",
        "\n",
        "#     for item in annotated_data:\n",
        "#         text = item['text']\n",
        "#         true_entities = item.get('entities', [])  # format: [(start, end, label)]\n",
        "\n",
        "#         pred_entities = predict_fn(text)  # format: [(start, end, label)]\n",
        "\n",
        "#         # Convert to label sets for span matching\n",
        "#         true_set = set((s, e, normalize_label(l, model=model_name.lower())) for s, e, l in true_entities)\n",
        "#         pred_set = set((s, e, normalize_label(l, model=model_name.lower())) for s, e, l in pred_entities)\n",
        "\n",
        "#         # Collect all possible labels\n",
        "#         labels = {\"PERSON\", \"LOC\", \"ORG\", \"EVENT\", \"MISC\"}\n",
        "\n",
        "#         # per-token tagging conversion\n",
        "#         for label in labels:\n",
        "#             true = [ent for ent in true_set if ent[2] == label]\n",
        "#             pred = [ent for ent in pred_set if ent[2] == label]\n",
        "\n",
        "#             TP = len(set(true) & set(pred))\n",
        "#             FP = len(set(pred) - set(true))\n",
        "#             FN = len(set(true) - set(pred)) # Fix: This should be set(true) - set(pred)\n",
        "\n",
        "#             all_true.append((label, TP, FP, FN))\n",
        "\n",
        "#     # Aggregate stats\n",
        "#     metrics = defaultdict(lambda: {\"TP\": 0, \"FP\": 0, \"FN\": 0})\n",
        "#     for label, TP, FP, FN in all_true:\n",
        "#         metrics[label][\"TP\"] += TP\n",
        "#         metrics[label][\"FP\"] += FP\n",
        "#         metrics[label][\"FN\"] += FN\n",
        "\n",
        "#     print(f\"=== Evaluation Report for {model_name.upper()} ===\")\n",
        "#     print(f\"{'Label':<10} {'Prec':<6} {'Recall':<6} {'F1':<6}\")\n",
        "#     label_metrics = {}\n",
        "#     for label in sorted(metrics):\n",
        "#         TP = metrics[label][\"TP\"]\n",
        "#         FP = metrics[label][\"FP\"]\n",
        "#         FN = metrics[label][\"FN\"]\n",
        "\n",
        "#         precision = TP / (TP + FP + 1e-8)\n",
        "#         recall = TP / (TP + FN + 1e-8)\n",
        "#         f1 = 2 * precision * recall / (precision + recall + 1e-8) if precision + recall else 0 # Fix: Avoid division by zero\n",
        "\n",
        "#         print(f\"{label:<10} {precision:.2f}  {recall:.2f}  {f1:.2f}\")\n",
        "#         label_metrics[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "#     return label_metrics\n",
        "\n",
        "\n",
        "def evaluate_model(model_name, predict_fn, annotated_data):\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    for item in annotated_data:\n",
        "        text = item['text']\n",
        "        true_entities = item.get('entities', [])  # format: [(start, end, label)]\n",
        "        pred_entities = predict_fn(text)  # format: [(start, end, label)]\n",
        "\n",
        "        # Normalize labels\n",
        "        true_set = set((s, e, normalize_label(l, model=model_name.lower())) for s, e, l in true_entities)\n",
        "        pred_set = set((s, e, normalize_label(l, model=model_name.lower())) for s, e, l in pred_entities)\n",
        "\n",
        "        # Debug: Print true and predicted entities\n",
        "        print(f\"Model: {model_name}, Text: {text}\")\n",
        "        print(f\"True entities: {true_set}\")\n",
        "        print(f\"Pred entities: {pred_set}\")\n",
        "\n",
        "        # Collect all possible labels\n",
        "        labels = {\"PERSON\", \"LOC\", \"ORG\", \"EVENT\", \"MISC\"}\n",
        "\n",
        "        # Per-label evaluation\n",
        "        for label in labels:\n",
        "            true = [ent for ent in true_set if ent[2] == label]\n",
        "            pred = [ent for ent in pred_set if ent[2] == label]\n",
        "\n",
        "            TP = len(set(true) & set(pred))\n",
        "            FP = len(set(pred) - set(true))\n",
        "            FN = len(set(true) - set(pred))\n",
        "\n",
        "            all_true.append((label, TP, FP, FN))\n",
        "\n",
        "    # Aggregate stats\n",
        "    metrics = defaultdict(lambda: {\"TP\": 0, \"FP\": 0, \"FN\": 0})\n",
        "    for label, TP, FP, FN in all_true:\n",
        "        metrics[label][\"TP\"] += TP\n",
        "        metrics[label][\"FP\"] += FP\n",
        "        metrics[label][\"FN\"] += FN\n",
        "\n",
        "    print(f\"=== Evaluation Report for {model_name.upper()} ===\")\n",
        "    print(f\"{'Label':<10} {'Prec':<6} {'Recall':<6} {'F1':<6}\")\n",
        "    label_metrics = {}\n",
        "    for label in sorted(metrics):\n",
        "        TP = metrics[label][\"TP\"]\n",
        "        FP = metrics[label][\"FP\"]\n",
        "        FN = metrics[label][\"FN\"]\n",
        "\n",
        "        precision = TP / (TP + FP + 1e-8)\n",
        "        recall = TP / (TP + FN + 1e-8)\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8) if (precision + recall) > 0 else 0\n",
        "\n",
        "        print(f\"{label:<10} {precision:.2f}  {recall:.2f}  {f1:.2f}\")\n",
        "        label_metrics[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "    return label_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVtJdywCLKYJ"
      },
      "outputs": [],
      "source": [
        "bert_results = evaluate_model(\"BERT\", predict_with_bert, annotated_data)\n",
        "flair_results = evaluate_model(\"FLAIR\", predict_with_flair, annotated_data)\n",
        "spacy_results = evaluate_model(\"SPACY\", predict_with_spacy, annotated_data)\n",
        "stanza_results = evaluate_model(\"STANZA\", predict_with_stanza, annotated_data)\n",
        "\n",
        "# print(annotated_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r32HRP77g07O"
      },
      "outputs": [],
      "source": [
        "for item in annotated_data[-10:]:\n",
        "    print(item['text'])\n",
        "    print(\"Entities:\", item['entities'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywT0w-PSeTrc"
      },
      "outputs": [],
      "source": [
        "# Aggregate results (from your code)\n",
        "all_results = {\n",
        "    \"BERT\": bert_results,\n",
        "    \"Flair\": flair_results,\n",
        "    \"spaCy\": spacy_results,\n",
        "    \"Stanza\": stanza_results\n",
        "}\n",
        "\n",
        "# Create dict of F1 scores per label per model\n",
        "f1_data = {}\n",
        "for model_name, label_metrics in all_results.items():\n",
        "    for label, scores in label_metrics.items():\n",
        "        if label not in f1_data:\n",
        "            f1_data[label] = {}\n",
        "        f1_data[label][model_name] = scores[\"f1\"]\n",
        "\n",
        "# Debug: Print f1_data to verify\n",
        "print(\"F1 Data:\", f1_data)\n",
        "\n",
        "# Check if f1_data is empty\n",
        "if not f1_data:\n",
        "    print(\"Error: No F1 scores available. Check evaluation results.\")\n",
        "else:\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(f1_data).T  # labels as index\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    # Debug: Print DataFrame to verify\n",
        "    print(\"DataFrame:\\n\", df)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    df.plot(\n",
        "        kind='bar',\n",
        "        ax=ax,\n",
        "        color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'],  # Distinct colors for BERT, Flair, spaCy, Stanza\n",
        "        alpha=0.8,\n",
        "        width=0.8\n",
        "    )\n",
        "    plt.title(\" NER Model F1 Scores by Label\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.xlabel(\"Entity Label\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.legend(title=\"Model\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"f1_by_label.png\", dpi=300, bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XX53RIHUds0"
      },
      "outputs": [],
      "source": [
        "# Heatmap\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(df, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=0.5)\n",
        "plt.title(\"NER Model F1 Score Heatmap\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Entity Label\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"f1_scores_heatmap.png\", dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zHGM1xhUrpl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels = list(df.index)\n",
        "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Complete the loop\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
        "\n",
        "for model in df.columns:\n",
        "    values = df[model].tolist()\n",
        "    values += values[:1]  # Complete the loop\n",
        "    ax.plot(angles, values, label=model)\n",
        "    ax.fill(angles, values, alpha=0.1)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(labels)\n",
        "plt.title('Model F1 Comparison (Radar Plot)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMbWDUFwUsWn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr66AZ29TcAR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16M5OvNPTkj9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyeJiR6vTqZU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X630U90UZk-D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define models and their predictions\n",
        "models = {\n",
        "    \"BERT\": predict_with_bert,\n",
        "    \"Flair\": predict_with_flair,\n",
        "    \"spaCy\": predict_with_spacy,\n",
        "    \"Stanza\": predict_with_stanza\n",
        "}\n",
        "\n",
        "# Tokenize helper\n",
        "def tokenize_text(text):\n",
        "    return text.split()  # Use a better tokenizer if needed\n",
        "\n",
        "# Convert span predictions to token-level labels\n",
        "def get_token_labels(text, spans):\n",
        "    tokens = tokenize_text(text)\n",
        "    labels = ['O'] * len(tokens)\n",
        "    for start, end, label in spans:\n",
        "        entity_text = text[start:end]\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token in entity_text and labels[i] == 'O':\n",
        "                labels[i] = label\n",
        "    return labels\n",
        "\n",
        "# Create dict of token labels for each model\n",
        "model_token_labels = defaultdict(list)\n",
        "\n",
        "for i, data in enumerate(annotated_data):\n",
        "    text = data[\"text\"]\n",
        "    for model_name, predict_fn in models.items():\n",
        "        spans = predict_fn(text) # Call the function with the text\n",
        "        token_labels = get_token_labels(text, spans)\n",
        "        model_token_labels[model_name].append(token_labels)\n",
        "\n",
        "# Compute agreement matrix\n",
        "agreement_df = pd.DataFrame(index=models.keys(), columns=models.keys())\n",
        "\n",
        "for m1 in models:\n",
        "    for m2 in models:\n",
        "        total = 0\n",
        "        agree = 0\n",
        "        for lbls1, lbls2 in zip(model_token_labels[m1], model_token_labels[m2]):\n",
        "            min_len = min(len(lbls1), len(lbls2))\n",
        "            total += min_len\n",
        "            agree += sum([1 for i in range(min_len) if lbls1[i] == lbls2[i]])\n",
        "        agreement = (agree / total) * 100 if total > 0 else 0\n",
        "        agreement_df.loc[m1, m2] = round(agreement, 2)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(agreement_df.astype(float), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Token-Level Agreement (%) Between NER Models\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"token_agreement.png\", dpi=300)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdlXYRSFivYD"
      },
      "outputs": [],
      "source": [
        "!pip -q install nbformat nbconvert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRLl9BJJlYAK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # follow the prompt\n",
        "NB_PATH = \"/content/drive/MyDrive/Colab Notebooks/aist_conf.ipynb\"  # ← change this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eNTEG46lmJU"
      },
      "outputs": [],
      "source": [
        "import nbformat as nbf\n",
        "from nbconvert.preprocessors import ClearOutputPreprocessor\n",
        "\n",
        "nb = nbf.read(NB_PATH, as_version=4)\n",
        "\n",
        "# Safest: drop the widgets metadata entirely (GitHub can’t use it anyway)\n",
        "if \"widgets\" in nb.metadata:\n",
        "    w = nb.metadata[\"widgets\"]\n",
        "    # if it's missing 'state' or looks odd, just remove it\n",
        "    if not isinstance(w, dict) or \"state\" not in w:\n",
        "        nb.metadata.pop(\"widgets\", None)\n",
        "\n",
        "# Also clear outputs to avoid heavy JSON\n",
        "cop = ClearOutputPreprocessor()\n",
        "nb, _ = cop.preprocess(nb, {})\n",
        "\n",
        "with open(NB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbf.write(nb, f)\n",
        "\n",
        "print(\"Cleaned & saved ✅\", NB_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wiDRpxKl0xs"
      },
      "outputs": [],
      "source": [
        "import glob, os, time\n",
        "cands = glob.glob(\"/content/**/*.ipynb\", recursive=True) + glob.glob(\"/content/drive/**/*.ipynb\", recursive=True)\n",
        "cands = sorted(cands, key=os.path.getmtime, reverse=True)[:15]\n",
        "for p in cands:\n",
        "    print(time.ctime(os.path.getmtime(p)), \"—\", p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUboogRUmm0x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBewuZTWo7DnKnxGxsks5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}